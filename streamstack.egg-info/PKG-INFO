Metadata-Version: 2.4
Name: streamstack
Version: 0.1.0
Summary: Production-grade LLM serving platform with FastAPI
Author: StreamStack Team
License: Apache-2.0
Project-URL: Homepage, https://github.com/alexyujiuqiao/streamstack
Project-URL: Repository, https://github.com/alexyujiuqiao/streamstack
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Internet :: WWW/HTTP :: HTTP Servers
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: fastapi>=0.104.0
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: pydantic>=2.4.0
Requires-Dist: pydantic-settings>=2.0.0
Requires-Dist: redis>=5.0.0
Requires-Dist: openai>=1.0.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: prometheus-client>=0.19.0
Requires-Dist: opentelemetry-api>=1.20.0
Requires-Dist: opentelemetry-sdk>=1.20.0
Requires-Dist: opentelemetry-instrumentation-fastapi>=0.41b0
Requires-Dist: opentelemetry-instrumentation-redis>=0.41b0
Requires-Dist: opentelemetry-instrumentation-httpx>=0.41b0
Requires-Dist: opentelemetry-exporter-prometheus>=1.12.0rc1
Requires-Dist: structlog>=23.1.0
Requires-Dist: asyncio-throttle>=1.0.2
Requires-Dist: async-timeout>=4.0.3
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.9.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.1.0; extra == "dev"
Requires-Dist: mypy>=1.6.0; extra == "dev"
Requires-Dist: locust>=2.17.0; extra == "dev"
Requires-Dist: httpx>=0.25.0; extra == "dev"
Provides-Extra: vllm
Requires-Dist: vllm>=0.2.0; extra == "vllm"
Dynamic: license-file

# StreamStack

A production-grade LLM serving platform built with FastAPI, featuring pluggable providers, advanced queuing, rate limiting, and comprehensive observability.

## Features

- **FastAPI-based API** with OpenAI-compatible `/v1/chat/completions` endpoint
- **Server-Sent Events (SSE)** support for streaming responses
- **Pluggable LLM Providers**: OpenAI, local vLLM, and custom providers
- **Advanced Queue Management**: Redis-based bounded queues with overflow handling
- **Rate Limiting**: Token-bucket algorithm with Redis backend
- **Idempotency**: Support for `Idempotency-Key` headers
- **Comprehensive Observability**:
  - Prometheus metrics (latencies, queue depth, throughput, costs)
  - OpenTelemetry distributed tracing
  - Grafana dashboards
- **Production Ready**: Health checks, graceful shutdown, structured logging
- **Load Testing**: Locust-based performance testing suite
- **Containerized Deployment**: Docker Compose with all services

## Quick Start

### Development Setup

```bash
# Clone the repository
git clone https://github.com/alexyujiuqiao/streamstack.git
cd streamstack

# Install dependencies
pip install -e ".[dev]"

# Start development server
uvicorn streamstack.main:app --reload --host 0.0.0.0 --port 8000
```

### Docker Compose Deployment

```bash
# Start all services (API, Redis, Prometheus, Grafana, vLLM)
docker-compose up -d

# View logs
docker-compose logs -f api

# Stop services
docker-compose down
```

## API Endpoints

- `POST /v1/chat/completions` - OpenAI-compatible chat completions (supports SSE)
- `GET /health` - Health check endpoint
- `GET /metrics` - Prometheus metrics endpoint

## Configuration

Configuration is managed through environment variables or a `.env` file:

```bash
# LLM Provider Settings
STREAMSTACK_PROVIDER=openai  # openai, vllm, or custom
OPENAI_API_KEY=your_api_key
VLLM_BASE_URL=http://localhost:8001

# Redis Configuration
REDIS_URL=redis://localhost:6379/0

# Rate Limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=100
RATE_LIMIT_TOKENS_PER_MINUTE=10000

# Queue Configuration
MAX_QUEUE_SIZE=1000
REQUEST_TIMEOUT=300

# Observability
ENABLE_TRACING=true
JAEGER_ENDPOINT=http://localhost:14268/api/traces
```

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=streamstack

# Run load tests
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

### Code Quality

```bash
# Format code
black streamstack tests
isort streamstack tests

# Lint
flake8 streamstack tests
mypy streamstack
```

## Monitoring

- **Grafana Dashboard**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **API Metrics**: http://localhost:8000/metrics

## Architecture

The system is designed with the following components:

1. **API Layer**: FastAPI application with SSE support
2. **Provider Layer**: Pluggable LLM providers (OpenAI, vLLM)
3. **Queue Layer**: Redis-based request queuing and rate limiting
4. **Observability Layer**: Metrics, tracing, and logging
5. **Infrastructure Layer**: Docker Compose orchestration

## License

Apache License 2.0 - see [LICENSE](LICENSE) file for details.
